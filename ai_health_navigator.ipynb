{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://platform.openai.com/usage #Usage Costs\n",
    "# https://cookbook.openai.com/examples/question_answering_using_embeddings\n",
    "# https://cookbook.openai.com/examples/embedding_wikipedia_articles_for_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast  # for converting embeddings saved as strings back to arrays\n",
    "from IPython.display import Markdown, display #displays ChatGPT's response neatly and cleanly\n",
    "import json # for configuration file\n",
    "import numpy as np\n",
    "from openai import OpenAI # for calling the OpenAI API\n",
    "import os # for loading directory files\n",
    "import pandas as pd  # for storing text and embeddings data\n",
    "from PyPDF2 import PdfReader # for reading PDF files\n",
    "import re # used for matching words and phrases for anonymization purposes\n",
    "from scipy import spatial  # for calculating vector similarities for search\n",
    "import tiktoken  # for counting tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see here for other model options: https://platform.openai.com/docs/models\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "GPT_MODEL = \"gpt-4-turbo\" #128,000 tokens\n",
    "\n",
    "if(EMBEDDING_MODEL == \"text-embedding-ada-002\"):\n",
    "    max_tokens = 32768\n",
    "if(EMBEDDING_MODEL == \"text-embedding-3-large\"):\n",
    "    max_tokens = 128000\n",
    "\n",
    "embeddings_filename = \"embeddings_\" + EMBEDDING_MODEL + \".csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load private variables from configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "    phi_words = config['phi_words']\n",
    "    ehr_directory = config['ehr_directory']\n",
    "    api_key = config['api_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get list of EHR files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_filenames(directory):\n",
    "    \"\"\"Lists all filenames in the given directory.\"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        return \"Directory does not exist.\"\n",
    "    \n",
    "    # List all files in the directory\n",
    "    filenames = [file for file in os.listdir(directory) if os.path.isfile(os.path.join(directory, file))]\n",
    "    return filenames\n",
    "\n",
    "# Get list of EHR filenames in EHR directory\n",
    "ehr_filenames = list_filenames(ehr_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load previously embedded EHR files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load previous paragraphs that have already been embedded if they exist, otherwise create a new dataframe with the correctly formatted columns\n",
    "if os.path.exists(embeddings_filename):\n",
    "    df_old_embeddings = pd.read_csv(embeddings_filename)    \n",
    "else:\n",
    "    df_old_embeddings = pd.DataFrame(columns=['text','embedding'])\n",
    "\n",
    "#convert the previously embedded to a list\n",
    "already_embedded = df_old_embeddings['text'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract, anonymize and embed text from new EHR files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymize_text(text, phi_words):\n",
    "    \"\"\"Anonymize text from phi_words loaded from configuration file\"\"\"\n",
    "    # Sort phi_words by length in descending order to avoid partial replacements\n",
    "    phi_words = sorted(phi_words, key=len, reverse=True)\n",
    "    for word in phi_words:\n",
    "        # Use regex to match the exact word or phrase\n",
    "        pattern = re.compile(re.escape(word), re.IGNORECASE)\n",
    "        text = pattern.sub('', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_string(string: str, model: str, max_tokens: int, print_warning: bool = True) -> str:\n",
    "    \"\"\"Truncate a string to a maximum number of tokens.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    encoded_string = encoding.encode(string)\n",
    "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
    "    if print_warning and len(encoded_string) > max_tokens:\n",
    "        print(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens.\")\n",
    "    return truncated_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
    "    \"\"\"Split a string in two, on a delimiter, trying to balance tokens on each side.\"\"\"\n",
    "    chunks = string.split(delimiter)\n",
    "    if len(chunks) == 1:\n",
    "        return [string, \"\"]  # no delimiter found\n",
    "    elif len(chunks) == 2:\n",
    "        return chunks  # no need to search for halfway point\n",
    "    else:\n",
    "        total_tokens = num_tokens(string)\n",
    "        halfway = total_tokens // 2\n",
    "        best_diff = halfway\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            left = delimiter.join(chunks[: i + 1])\n",
    "            left_tokens = num_tokens(left)\n",
    "            diff = abs(halfway - left_tokens)\n",
    "            if diff >= best_diff:\n",
    "                break\n",
    "            else:\n",
    "                best_diff = diff\n",
    "        left = delimiter.join(chunks[:i])\n",
    "        right = delimiter.join(chunks[i:])\n",
    "        return [left, right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_paragraphs_into_sections(paragraph: str, max_tokens: int = 1000, model: str = GPT_MODEL, max_recursion: int = 5) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split a paragraph into a list of subsections no longer than max_tokens.\n",
    "    Each subsection is a tuple of parent titles [H1, H2, ...] and text (str).\n",
    "    \"\"\"    \n",
    "    num_tokens_in_string = num_tokens(paragraph)\n",
    "    \n",
    "    # if length is fine, return string\n",
    "    if num_tokens_in_string <= max_tokens:\n",
    "        return [paragraph]\n",
    "    # if recursion hasn't found a split after X iterations, just truncate\n",
    "    elif max_recursion == 0:\n",
    "        return [truncated_string(paragraph, model=model, max_tokens=max_tokens)]\n",
    "    # otherwise, split in half and recurse\n",
    "    else:\n",
    "        # titles, text = subsection\n",
    "        for delimiter in [\"\\n\\n\", \"\\n\", \". \"]:\n",
    "            left, right = halved_by_delimiter(paragraph, delimiter=delimiter)\n",
    "            if left == \"\" or right == \"\":\n",
    "                # if either half is empty, retry with a more fine-grained delimiter\n",
    "                continue\n",
    "            else:\n",
    "                # recurse on each half\n",
    "                results = []\n",
    "                for half in [left, right]:\n",
    "                    # half_subsection = (titles, half)\n",
    "                    half_strings = split_paragraphs_into_sections(half, max_tokens=max_tokens, model=model, max_recursion=max_recursion - 1)\n",
    "                    results.extend(half_strings)\n",
    "                return results\n",
    "    # otherwise no split was found, so just truncate (should be very rare)\n",
    "    return [truncated_string(paragraph, model=model, max_tokens=max_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_sections = []\n",
    "# gather text from each EHR file in the EHR folder\n",
    "for filename in ehr_filenames:\n",
    "    reader = PdfReader(os.path.join(ehr_directory, filename))\n",
    "\n",
    "    # Load all pages in pdf file\n",
    "    for page in reader.pages:\n",
    "\n",
    "        # get the text from each individual page\n",
    "        text = page.extract_text()\n",
    "\n",
    "        # split into paragraphes from each page of text\n",
    "        paragraphs = text.split(\"\\n\\n\")\n",
    "\n",
    "        # run through every paragraph\n",
    "        for paragraph in paragraphs:\n",
    "\n",
    "            # check to see if every paragraph is within the model's token limit\n",
    "            paragraph_splits = split_paragraphs_into_sections(paragraph, max_tokens)\n",
    "\n",
    "            # run through every paragraph or section of paragraph\n",
    "            for paragraph_split in paragraph_splits:\n",
    "\n",
    "                # anonymize the text\n",
    "                anonymized_line = anonymize_text(paragraph_split, phi_words)\n",
    "\n",
    "                # if the anonymized line isn't in the list of previously embeddeded texts add it to the list to be embedded\n",
    "                if(anonymized_line not in already_embedded):\n",
    "                    all_text_sections.append(anonymized_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create openAI client for embeddings and later querying ChatGPT\n",
    "client = OpenAI(api_key=api_key)\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "embeddings = []\n",
    "for batch_start in range(0, len(all_text_sections), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = all_text_sections[batch_start:batch_end]\n",
    "    print(f\"Batch {batch_start} to {batch_end-1}\")\n",
    "    response = client.embeddings.create(model=EMBEDDING_MODEL, input=batch)\n",
    "    for i, be in enumerate(response.data):\n",
    "        assert i == be.index  # double check embeddings are in same order as input\n",
    "    batch_embeddings = [e.embedding for e in response.data]\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n",
    "df_new_embeddings = pd.DataFrame({\"text\": all_text_sections, \"embedding\": embeddings})\n",
    "df_embeddings = pd.concat([df_old_embeddings, df_new_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save embeddings so that they do not have to be re-embedded for future uses\n",
    "df_embeddings.to_csv(embeddings_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search function\n",
    "def strings_ranked_by_relatedness(query: str, df: pd.DataFrame, relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y), top_n: int = 100) -> tuple[list[str], list[float]]:\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    query_embedding_response = client.embeddings.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=query,\n",
    "    )\n",
    "    query_embedding = query_embedding_response.data[0].embedding\n",
    "\n",
    "    # Ensure the embeddings are in numeric format\n",
    "    df['embedding'] = df['embedding'].apply(lambda emb: np.array(eval(emb)) if isinstance(emb, str) else np.array(emb))\n",
    "\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n], relatednesses[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_message(query: str, df: pd.DataFrame, model: str, token_budget: int) -> str:\n",
    "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
    "    input_texts, relatednesses = strings_ranked_by_relatedness(query, df)\n",
    "    introduction = 'Please answer questions as an expert in medicine.'\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "    message = introduction\n",
    "    for input_text in input_texts:\n",
    "        if (\n",
    "            num_tokens(message + input_text + question, model=model)\n",
    "            > token_budget\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            message += input_text\n",
    "    return message + question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query: str, df: pd.DataFrame = df_embeddings, model: str = GPT_MODEL, token_budget: int = 4096 - 500, print_message: bool = False) -> str:\n",
    "    \"\"\"Answers a query using GPT and a dataframe of relevant texts and embeddings.\"\"\"\n",
    "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
    "    if print_message:\n",
    "        print(message)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Please answer prompts as an expert in medicine.\"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response.choices[0].message.content\n",
    "    return response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Please summarize this patient's medical history and general state of health. What are the patient's most pressing medical concerns? What course of treatment do you recommened?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Summary of Medical History and General State of Health:**\n",
       "\n",
       "The patient is a 70-year-old male with a complex medical history that includes:\n",
       "- Chronic conditions such as psoriasis, hyperlipidemia, essential hypertension, and obstructive sleep apnea.\n",
       "- History of skin cancer (basal cell carcinoma excised in 2013) and ongoing skin issues like benign nevus and seborrheic keratosis.\n",
       "- Cardiovascular issues including coronary artery disease with stable angina, atrial tachycardia, and a history of carotid endarterectomy and coronary stent placements.\n",
       "- Neurological concerns such as intractable chronic migraine and a history of cryptogenic stroke.\n",
       "- Orthopedic issues including bilateral knee replacements and degenerative disc disease.\n",
       "\n",
       "The patient's general state of health appears managed through multiple medications and regular medical follow-ups, though the complexity of his conditions requires careful monitoring and coordination of care.\n",
       "\n",
       "**Most Pressing Medical Concerns:**\n",
       "1. **Cardiovascular Health:** Given his history of coronary artery disease, atrial tachycardia, and carotid artery stenosis, maintaining cardiovascular health is crucial. His recent procedures like cardiac catheterization and EP study indicate active management but necessitate ongoing vigilance.\n",
       "2. **Skin Cancer Surveillance:** With a history of basal cell carcinoma and ongoing new lesions of seborrheic keratosis, regular dermatological evaluations are essential to monitor for potential malignant changes.\n",
       "3. **Psoriasis Management:** Chronic psoriasis requires ongoing treatment to manage symptoms and prevent flare-ups.\n",
       "4. **Migraine Management:** Chronic migraines can significantly impact quality of life and require effective management strategies.\n",
       "\n",
       "**Recommended Course of Treatment:**\n",
       "1. **Cardiovascular Management:**\n",
       "   - Continue current medications including aspirin, ezetimibe, and Repatha for lipid management.\n",
       "   - Regular follow-up with a cardiologist to monitor heart function and adjust treatments as necessary.\n",
       "   - Maintain lifestyle modifications including diet, exercise, and stress management.\n",
       "\n",
       "2. **Dermatological Care:**\n",
       "   - Regular skin examinations to monitor existing conditions and check for new lesions.\n",
       "   - Continue with prescribed treatments for psoriasis and manage seborrheic keratosis as recommended by the dermatologist.\n",
       "   - Emphasize the importance of photoprotection to reduce the risk of new skin cancers.\n",
       "\n",
       "3. **Neurological Support:**\n",
       "   - Optimize migraine management possibly through adjustments in medication or exploring new treatment options if current regimen is insufficient.\n",
       "   - Monitor for neurological symptoms that could suggest vascular issues or stroke recurrence.\n",
       "\n",
       "4. **General Health Maintenance:**\n",
       "   - Continue with regular screenings and vaccinations as recommended for age and medical history.\n",
       "   - Address lifestyle factors such as diet, exercise, and weight management to support overall health.\n",
       "   - Regular follow-ups with primary care to coordinate care across specialties and adjust treatments as needed.\n",
       "\n",
       "5. **Mental and Emotional Health:**\n",
       "   - Consider regular assessments for mood and cognitive function, given the extensive medical history and potential stressors associated with chronic illness management.\n",
       "\n",
       "This comprehensive approach should help manage the patient's complex health needs and maintain his quality of life. Regular communication between the patient and his healthcare providers is crucial to effectively address his medical concerns."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ask(query)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the 5 most related strings according to cosine similarity\n",
    "strings, relatednesses = strings_ranked_by_relatedness(query, df_embeddings, top_n=5)\n",
    "for string, relatedness in zip(strings, relatednesses):\n",
    "    print(f\"{relatedness=:.3f}\")\n",
    "    display(Markdown(string))\n",
    "    print(\"--------------------------------\")\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of tokens for all submitted strings\n",
    "total = 0\n",
    "for string in strings:\n",
    "    print(num_tokens(string))\n",
    "    total += num_tokens(string)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f8929016d30f12a89eab5b5e02a6c1410fc73da4b23b89b4eb7a3bc58137fe5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
